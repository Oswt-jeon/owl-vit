import torch
import cv2
from PIL import Image
from transformers import Owlv2Processor, Owlv2ForObjectDetection
import os
import math

# -------------------------------
# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
# -------------------------------
SCORE_THRESH   = 0.75   # 1Ï∞® Ïª∑
NMS_THRESH     = 0.3
TOPK           = None   # ‚Üê Ïö∞ÏÑ† NoneÏúºÎ°ú ÎëêÍ≥† Ïã§Ï†ú Í∞úÏàòÎ∂ÄÌÑ∞ Î≥¥Ïûê. (ÌïÑÏöîÌïòÎ©¥ 10~20ÏúºÎ°ú)
MIN_AREA_RATIO = 0.003  # 0.3%Î°ú ÏÉÅÌñ• (Ïû•Î©¥Ïóê Îî∞Îùº 0.005ÍπåÏßÄÎèÑ)
MAX_AR         = 4.0    # Ï¢ÖÌö°ÎπÑ Ï†úÌïú Í∞ïÌôî
RADIUS_NMS_PX  = 48     # Ï§ëÏã¨Í±∞Î¶¨ Í∏∞Î∞ò Ï∂îÍ∞Ä ÏñµÏ†ú(Ìï¥ÏÉÅÎèÑ Îî∞Îùº 32~96Î°ú Ï°∞Ï†ï)
DYN_ALPHA      = 1.0    # ÎèôÏ†Å Ïä§ÏΩîÏñ¥ Ïª∑: mean + alpha*std

# -------------------------------
# Î™®Îç∏ ÏÑ§Ï†ï (Ensemble Ìï¥Ï†ú)
# -------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model_id = "google/owlv2-base-patch16"   # ‚Üê ensembleÏù¥ ÏïÑÎãå Îã®Ïùº Î™®Îç∏

processor = Owlv2Processor.from_pretrained(model_id)
model = Owlv2ForObjectDetection.from_pretrained(model_id).to(device).eval()

# -------------------------------
# Í≤ΩÎ°ú
# -------------------------------
target_img_path = "images/target3.jpeg"
query_img_path  = "images/query5.jpeg"
output_path     = "images/result.jpg"

if not os.path.exists(target_img_path):
    raise FileNotFoundError(f"‚ùå target Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {target_img_path}")
if not os.path.exists(query_img_path):
    raise FileNotFoundError(f"‚ùå query Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {query_img_path}")

# -------------------------------
# Î°úÎìú
# -------------------------------
target_img = Image.open(target_img_path).convert("RGB")
query_img  = Image.open(query_img_path).convert("RGB")
H, W = target_img.height, target_img.width
IMG_AREA = float(H * W)
MIN_AREA = IMG_AREA * MIN_AREA_RATIO

# -------------------------------
# Ï∂îÎ°†
# -------------------------------
inputs = processor(
    images=target_img,
    query_images=[query_img],
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model.image_guided_detection(**inputs)

res = processor.post_process_image_guided_detection(
    outputs=outputs,
    target_sizes=torch.tensor([(H, W)], device=device),
    threshold=SCORE_THRESH,
    nms_threshold=NMS_THRESH
)[0]

boxes: torch.Tensor  = res["boxes"]
scores: torch.Tensor = res["scores"]

# -------------------------------
# (1) ÌòïÌÉú ÌïÑÌÑ∞: ÏµúÏÜå Î©¥Ï†Å & Ï¢ÖÌö°ÎπÑ
# -------------------------------
if boxes.numel() > 0:
    x1, y1, x2, y2 = boxes.unbind(dim=1)
    w = (x2 - x1).clamp(min=0)
    h = (y2 - y1).clamp(min=0)
    area = w * h
    ar = torch.maximum(w / (h + 1e-6), h / (w + 1e-6))
    keep = (area >= MIN_AREA) & (ar <= MAX_AR)
    boxes = boxes[keep]
    scores = scores[keep]

# -------------------------------
# (2) ÎèôÏ†Å Ïä§ÏΩîÏñ¥ Ïª∑: mean+alpha*std
# -------------------------------
if scores.numel() > 0:
    mean = scores.mean()
    std  = scores.std(unbiased=False)
    dyn_thresh = max(SCORE_THRESH, float(mean + DYN_ALPHA * std))
    keep = scores >= dyn_thresh
    boxes = boxes[keep]
    scores = scores[keep]

# -------------------------------
# (3) Containment/IoA ÏñµÏ†ú
#    IoU ÎßêÍ≥† "Í±∞Ïùò ÏôÑÏ†ÑÌûà Ìè¨Ìï®(ÌÅ∞ Î∞ïÏä§ ÏïàÏóê Îì§Ïñ¥Í∞ê)"ÎèÑ Ï†úÍ±∞
# -------------------------------
def containment_suppression(b, s, ioa_thr=0.9):
    # IoA: A‚à©B / A  (ÏûëÏùÄ Î∞ïÏä§ ÏûÖÏû•ÏóêÏÑú ÎåÄÎ∂ÄÎ∂ÑÏù¥ ÌÅ∞ Î∞ïÏä§Ïóê Ìè¨Ìï®ÎêòÎ©¥ Ï†úÍ±∞)
    if b.numel() == 0:
        return b, s
    idx = torch.argsort(s, descending=True)
    b = b[idx]
    s = s[idx]
    keep = []
    for i in range(b.size(0)):
        bi = b[i]
        x1i, y1i, x2i, y2i = bi
        wi = (x2i - x1i).clamp(min=0)
        hi = (y2i - y1i).clamp(min=0)
        areai = wi * hi + 1e-6
        contained = False
        for j in keep:
            bj = b[j]
            xx1 = torch.maximum(x1i, bj[0])
            yy1 = torch.maximum(y1i, bj[1])
            xx2 = torch.minimum(x2i, bj[2])
            yy2 = torch.minimum(y2i, bj[3])
            inter = torch.clamp(xx2 - xx1, min=0) * torch.clamp(yy2 - yy1, min=0)
            ioa = inter / areai  # A Í∏∞Ï§Ä
            if ioa > ioa_thr:
                contained = True
                break
        if not contained:
            keep.append(i)
    return b[keep], s[keep]

boxes, scores = containment_suppression(boxes, scores, ioa_thr=0.9)

# -------------------------------
# (4) Radius-NMS (ÏÑºÌÑ∞ Í±∞Î¶¨ Í∏∞Î∞ò Ï§ëÎ≥µ ÏñµÏ†ú)
# -------------------------------
def radius_nms(b, s, radius_px=RADIUS_NMS_PX):
    if b.numel() == 0:
        return b, s
    idx = torch.argsort(s, descending=True)
    b = b[idx]
    s = s[idx]
    keep = []
    centers = torch.stack([(b[:, 0] + b[:, 2]) * 0.5, (b[:, 1] + b[:, 3]) * 0.5], dim=1)
    for i in range(b.size(0)):
        ci = centers[i]
        drop = False
        for j in keep:
            cj = centers[j]
            dx = float(ci[0] - cj[0])
            dy = float(ci[1] - cj[1])
            if (dx * dx + dy * dy) <= (radius_px * radius_px):
                drop = True
                break
        if not drop:
            keep.append(i)
    return b[keep], s[keep]

boxes, scores = radius_nms(boxes, scores, radius_px=RADIUS_NMS_PX)

# -------------------------------
# (5) ÏµúÏ¢Ö Top-K (ÏõêÌïòÎ©¥ ÏÇ¨Ïö©)
# -------------------------------
if TOPK is not None and boxes.size(0) > TOPK:
    topk_idx = torch.topk(scores, k=TOPK, largest=True, sorted=True).indices
    boxes = boxes[topk_idx]
    scores = scores[topk_idx]

# -------------------------------
# ÏãúÍ∞ÅÌôî
# -------------------------------
vis = cv2.cvtColor(cv2.imread(target_img_path), cv2.COLOR_BGR2RGB)
for box, score in zip(boxes, scores):
    x1, y1, x2, y2 = map(int, box.tolist())
    label = f"query {float(score):.2f}"
    cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 3)
    cv2.putText(vis, label, (x1, max(0, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX,
                0.8, (255, 255, 255), 2, lineType=cv2.LINE_AA)
    cv2.putText(vis, label, (x1, max(0, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX,
                0.8, (0, 0, 0), 1, lineType=cv2.LINE_AA)

cv2.imwrite(output_path, cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))
print(f"‚úÖ ÏµúÏ¢Ö ÌÉêÏßÄ Ïàò: {boxes.size(0)}")
print(f"üíæ Í≤∞Í≥º Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏúÑÏπò: {output_path}")
